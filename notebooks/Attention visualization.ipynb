{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22148d5",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from AMPpred_MFA.lib.Encoding import easy_encoding\n",
    "from AMPpred_MFA.lib.Vocab import load_vocab\n",
    "from AMPpred_MFA.lib.Data import load_dataset, get_data_iter, get_data_loader\n",
    "from AMPpred_MFA.models.Model import load_model\n",
    "from AMPpred_MFA.models.AMPpred_MFA import Model, Config\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "save_path_attention_visual = '../figures/attention visualization/A0A1P8AQ95' # Saving path\n",
    "os.makedirs(save_path_attention_visual, exist_ok=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c68c99",
   "metadata": {},
   "source": [
    "vocab_path = './trained_model/vocab.json'\n",
    "model_path = './trained_model/model.pth'\n",
    "vocab = load_vocab(vocab_path)\n",
    "config = Config()\n",
    "config.k_mer = 1\n",
    "config.batch_size = 32\n",
    "config.embed_padding_idx = vocab[config.padding_token]\n",
    "config.feature_dim = 400\n",
    "config.vocab_size = len(vocab)\n",
    "model = Model(config)\n",
    "load_model(model, model_path)\n",
    "model.eval()\n",
    "fastas = np.array([\n",
    "    ('A0A1P8AQ95','MTKNMTKKKMGLMSPNIAAFVLPMLLVLFTISSQVEVVESTGRKLSWAFNGAPIVFTPPSSSCGGSPAAVMASEWMPRRPCRRTRPPGTNIPVSQSP'),\n",
    "    ])\n",
    "data_x = easy_encoding(fastas, 'mixed', vocab,\n",
    "                        config.k_mer, config.padding_size)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57621a7b",
   "metadata": {},
   "source": [
    "out = model(data_x)\n",
    "attention2 = model.attention_wight2.cpu().detach().numpy()\n",
    "attention2_inputs = model.attention_wight2_inputs.cpu().detach().numpy().reshape(config.padding_size, -1)\n",
    "attention2_outputs = model.attention_wight2_outputs.cpu().detach().numpy().reshape(config.padding_size, -1)\n",
    "print(attention2.shape)\n",
    "print(attention2_inputs.shape)\n",
    "print(attention2_outputs.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba38c24",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def show_attention_heatmap(attention_weights: np.ndarray):\n",
    "    \"\"\"\n",
    "    Heatmap visualization of multi-head attention weight matrix\n",
    "     'viridis': blue-white-yellow, gradually darkening\n",
    "     'plasma': purple-yellow, progressively darker\n",
    "     'inferno': black, red, yellow and white, progressively darker\n",
    "    Args:\n",
    "        attention_weights: np.ndarray, Attention weight matrix, shape is (num_heads, seq_length, seq_length)\n",
    "    \"\"\" \n",
    "    num_heads, seq_length, _ = attention_weights.shape\n",
    "    # Calculate the number of images to be displayed and their arrangement, try to display them in a square\n",
    "    num_cols = int(np.ceil(np.sqrt(num_heads)))\n",
    "    num_rows = int(np.ceil(num_heads / num_cols))\n",
    "    # Create a subgraph of num_rows * num_cols\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(num_cols * 8, num_rows * 8))\n",
    "    # Flatten the axis to facilitate subsequent traversal\n",
    "    axs = axs.flatten()\n",
    "    for i, ax in enumerate(axs):\n",
    "        head_attention = attention_weights[i, :, :]\n",
    "        im = ax.imshow(head_attention, cmap='viridis')\n",
    "        ax.set_xticks(range(0, seq_length, 3),range(0 ,seq_length, 3), fontsize=14,rotation=90)\n",
    "        ax.set_yticks(range(0, seq_length, 3),range(0 ,seq_length, 3), fontsize=14)\n",
    "        ax.tick_params(axis='both')\n",
    "        ax.set_xlabel('Output position')\n",
    "        ax.set_ylabel('Input position')\n",
    "        ax.set_title('Attention head {}'.format(i+1))\n",
    "        # Add color bar\n",
    "        cbar = ax.figure.colorbar(im, ax=ax, shrink=0.7)\n",
    "        # cbar.ax.set_ylabel('Attention weight', va='center', labelpad=10, rotation=-90)\n",
    "    # Delete unused subgraphs\n",
    "    for i in range(num_rows * num_cols - num_heads):\n",
    "        fig.delaxes(axs[-1,-(i+1)])\n",
    "    # fig.tight_layout()\n",
    "    fig.subplots_adjust(left=0, bottom=0.09, right=0.9, top=0.9, wspace=0.16, hspace=0)\n",
    "    return fig\n",
    "\n",
    "mpl.rcParams.update({'font.size': 20})  \n",
    "fig_attention_heatmap = show_attention_heatmap(attention2)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f78518",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "def show_attention_feature_ranking(attention_weights: np.ndarray, num_features=20):\n",
    "    \"\"\"\n",
    "    Feature Importance Ranking for Attention Weights\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: np.ndarray, Attention weight matrix, the shape is (num_heads, seq_length, seq_length)\n",
    "        num_features: int, Show first num_features features\n",
    "    \"\"\" \n",
    "    # Averaging the multi-head attention weight matrix\n",
    "    avg_weights = np.mean(attention_weights, axis=0)\n",
    "    # Calculate the output attention value of the current position (sum the input dimension to get the attention value of the output dimension)\n",
    "    output_weights = np.sum(avg_weights, axis=0)\n",
    "    # Sort features in descending order by attention value\n",
    "    sorted_idx = np.argsort(output_weights)[::-1][:num_features]\n",
    "    sorted_features = output_weights[sorted_idx]\n",
    "    # Draw histogram\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.bar(np.arange(num_features), sorted_features)\n",
    "    ax.set_xticks(np.arange(num_features),[i for i in sorted_idx])\n",
    "    # ax.spines['right'].set_visible(False)\n",
    "    # ax.spines['top'].set_visible(False)\n",
    "    ax.tick_params(axis='both')\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Attention value')\n",
    "    ax.set_title('Top {} of attention values'.format(num_features))\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "mpl.rcParams.update({'font.size': 12})  \n",
    "fig_feature_ranking = show_attention_feature_ranking(attention2, 30)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf30a28",
   "metadata": {},
   "source": [
    "def show_attention_distribution(attention_weights: np.ndarray):\n",
    "    \"\"\"\n",
    "    Boxplots for visualizing the output of multi-head attention\n",
    "    \n",
    "    Args:\n",
    "    attention_weights: numpy array, multi-head attention weight matrix, shape is (num_heads, seq_length, seq_length)\n",
    "    \"\"\"\n",
    "    num_heads, seq_length, _ = attention_weights.shape\n",
    "    # Sum over the input dimension to get the attention of the output position under different heads\n",
    "    output_weights = attention_weights.sum(axis=1)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axs = axs.flatten()\n",
    "    fig.suptitle('Output feature distribution')\n",
    "    # Boxplot\n",
    "    sns.boxplot(data=output_weights.T, ax=axs[0])\n",
    "    axs[0].set_title('Head-wise attention distribution')\n",
    "    axs[0].set_xticks(np.arange(0,4), np.arange(1,5))\n",
    "    axs[0].set_xlabel('Attention head')\n",
    "    axs[0].set_ylabel('Attention value')\n",
    "    # Barplot\n",
    "    axs[1].bar(np.arange(seq_length), output_weights.mean(axis=0))\n",
    "    axs[1].set_title('Position-wise attention distribution')\n",
    "    axs[1].set_xlabel('Postion')\n",
    "    axs[1].set_ylabel('Attention value')\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.2)\n",
    "    return fig\n",
    "\n",
    "mpl.rcParams.update({'font.size': 14})  \n",
    "fig_output = show_attention_distribution(attention2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f62dcb33",
   "metadata": {},
   "source": [
    "def show_attention_input_output(inputs: np.ndarray,outputs: np.ndarray):\n",
    "    \"\"\"\n",
    "    Visualizing the output matrix and output matrix of multi-head attention\n",
    "    \n",
    "    Args:\n",
    "    inputs: numpy array, the input matrix of the multi-head attention layer, shape is (seq_length, embedding_size)\n",
    "    outputs: numpy array, the output matrix of the multi-head attention layer, the shape is (seq_length, embedding_size)\n",
    "    \"\"\"\n",
    "    seq_length, embedding_size = inputs.shape\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(6, 10))\n",
    "    axs = axs.flatten()\n",
    "    # Plot a heatmap of the input matrix\n",
    "    sns.heatmap(inputs.T, cmap='viridis', ax=axs[0])\n",
    "    axs[0].set_title('Original features before processing by the attention layer')\n",
    "    axs[0].set_xlabel('Position')\n",
    "    axs[0].set_ylabel('Embedding')\n",
    "    axs[0].set_xticks(range(0, seq_length, 5),range(0, seq_length, 5), fontsize=10)\n",
    "    axs[0].set_yticks(range(0, embedding_size, 5),range(0, embedding_size, 5), fontsize=10)\n",
    "    # Plot a heatmap of the output matrix\n",
    "    sns.heatmap(outputs.T, cmap='viridis', ax=axs[1])\n",
    "    axs[1].set_title('Implicated features after processing by the attention layer')\n",
    "    axs[1].set_xlabel('Position')\n",
    "    axs[1].set_ylabel('Embedding')\n",
    "    axs[1].set_xticks(range(0, seq_length, 5),range(0, seq_length, 5), fontsize=10)\n",
    "    axs[1].set_yticks(range(0, embedding_size, 5),range(0, embedding_size, 5), fontsize=10)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "mpl.rcParams.update({'font.size': 14})  \n",
    "fig_input_output_features = show_attention_input_output(attention2_inputs, attention2_outputs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4024335",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "import random\n",
    "def normalize(values):\n",
    "    \"\"\"\n",
    "    Normalize the incoming list\n",
    "    \"\"\"\n",
    "    min_val = min(values)  \n",
    "    max_val = max(values) \n",
    "    if min_val == max_val: \n",
    "        return [1.0] * len(values)\n",
    "    else:\n",
    "        return [(val - min_val) / (max_val - min_val) for val in values]  \n",
    "\n",
    "def generate_random_color():\n",
    "    \"\"\"\n",
    "    Generate random rgb tuples\n",
    "    \"\"\"\n",
    "    r, g, b = np.random.randint(0,255,size=3)\n",
    "    # Limit luma brightness value above 70\n",
    "    while 0.2126 * r + 0.7152 * g + 0.0722 * b > 70:\n",
    "        r, g, b = np.random.randint(0,255,size=3)\n",
    "    return (r/255, g/255, b/255) \n",
    "\n",
    "def show_attention_motifs(attention_weights, sequence, k_mer=1):\n",
    "    \"\"\"\n",
    "    Visual attention motifs\n",
    "    \n",
    "    Args:\n",
    "    attention_weights: numpy array, multi-head attention weight matrix, shape is (num_heads, seq_length, seq_length)\n",
    "    sequence: str, the protein sequence\n",
    "    k_mer: int, k value of k_mer\n",
    "    \"\"\"\n",
    "    # Compute the attention score for each location\n",
    "    avg_attention_weights = attention_weights.mean(axis=0)\n",
    "    attention_scores = avg_attention_weights.sum(axis=0)\n",
    "    attention_scores = normalize(attention_scores)\n",
    "    # Divide the sequence into several k-mer fragments\n",
    "    motif_names = [sequence[i:i+k_mer] for i in range(len(sequence)-k_mer-1)]\n",
    "    num_motifs = len(motif_names)\n",
    "    # Calculate the height at which each motif should be displayed\n",
    "    motif_size = attention_scores / np.max(attention_scores) * 100\n",
    "    # Initialize image\n",
    "    fig, ax = plt.subplots(figsize=(20,4)) \n",
    "    color_dict = {}\n",
    "    for i in range(num_motifs):\n",
    "        # Motif vertical formatting\n",
    "        motif = '\\n'.join(list(motif_names[i]))\n",
    "        if attention_scores[i] in color_dict:\n",
    "            color = color_dict[attention_scores[i]]\n",
    "        else:\n",
    "            color =generate_random_color()\n",
    "            color_dict[attention_scores[i]] = color\n",
    "        ax.text(i, 0, motif, ha='center', va='bottom',  fontsize=motif_size[i], color=color)\n",
    "    ax.set_xlim(0, num_motifs)\n",
    "    ax.set_ylim(0, k_mer)\n",
    "    ax.axis('off')\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig_motif = show_attention_motifs(attention2, fastas[0][1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e28a7acf",
   "metadata": {},
   "source": [
    "def show_attention_network(attention_weights: np.ndarray):\n",
    "    \"\"\"\n",
    "    Attention Network Visualization\n",
    "    Args:\n",
    "        attention_weights: np.ndarray, Attention weight matrix, the shape is (num_heads, seq_length, seq_length)\n",
    "    \"\"\" \n",
    "    num_heads, seq_length, _ = attention_weights.shape\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(6*4, 10),facecolor='black')\n",
    "\n",
    "    fig.suptitle('Network of attention')\n",
    "    colors = ['r','g','b','orange']\n",
    "    for i, ax in enumerate(axs):\n",
    "        head_attention = attention_weights[i, :, :]\n",
    "        # Calculate the maximum attention index for each output position\n",
    "        attention = head_attention.argmax(axis=1)\n",
    "        unique_elements, counts = np.unique(attention, return_counts=True)\n",
    "        # Set line transparency\n",
    "        freq_dict = dict(zip(unique_elements, counts/max(counts)))\n",
    "        for y,idx in enumerate(attention):\n",
    "            ax.plot([1,4],[98-y,idx],alpha=freq_dict[idx],color=colors[i],linewidth=2)\n",
    "        ax.set_title('Attention head {}'.format(i+1), color = 'white')\n",
    "        ax.set_xlim(0, 5)\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.axis('off')    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "mpl.rcParams.update({'font.size': 28})  \n",
    "fig_attention_network = show_attention_network(attention2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "240ee41e",
   "metadata": {},
   "source": [
    "fig_attention_heatmap.savefig(os.path.join(save_path_attention_visual, 'attention heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "fig_feature_ranking.savefig(os.path.join(save_path_attention_visual, 'feature ranking.png'), dpi=300, bbox_inches='tight')\n",
    "fig_output.savefig(os.path.join(save_path_attention_visual, 'output feature distribution.png'), dpi=300, bbox_inches='tight')\n",
    "fig_input_output_features.savefig(os.path.join(save_path_attention_visual, 'inputs and outputs features.png'), dpi=300, bbox_inches='tight')\n",
    "fig_motif.savefig(os.path.join(save_path_attention_visual, 'motif.png'), dpi=300, bbox_inches='tight')\n",
    "fig_attention_network.savefig(os.path.join(save_path_attention_visual, 'attention network.png'), dpi=300, bbox_inches='tight')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93592a3a",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
